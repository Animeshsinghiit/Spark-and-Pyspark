{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "261b9ad0",
   "metadata": {},
   "source": [
    "# Advanced RDD Transformations and Actions\n",
    "               - ANIMESH SINGH\n",
    "> FOR PART 1 OF THIS NOTEBOOK REFER TO THIS LINK : [Part1](https://github.com/Animeshsinghiit/Spark-and-Pyspark/blob/main/pyspark%20practise%20part%201.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c426e190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext,SparkConf\n",
    "conf=SparkConf().setAppName('RDD_practise').setMaster(\"local[*]\")\n",
    "sc=SparkContext(conf=conf)\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f817d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "randlist=random.sample(range(0,40),10)\n",
    "randlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b5ffa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1=sc.parallelize(randlist,4)\n",
    "rdd2=rdd1.map(lambda x:x+3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1230e1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('rdd1',rdd1.collect())\n",
    "print('rdd2',rdd2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fafcb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# union()\n",
    "rdd2=rdd1.union(rdd2)\n",
    "print(\"partitions :\",rdd2.glom().collect())\n",
    "print(\"Total number of partitions\",rdd2.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d3271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intersection()\n",
    "rdd3=rdd1.intersection(rdd2)\n",
    "print(\"partitions \",rdd3.glom().collect())\n",
    "print(\"Number of partions \",rdd3.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803fe8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Number of Empty Partitions\n",
    "count=0\n",
    "for item in rdd3.glom().collect():\n",
    "    if len(item)==0:\n",
    "        count+=1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaabe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coalesce(num of partions) : used to decrease number of partitions\n",
    "rdd3.coalesce(2).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8165e604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takeSample(withreplacement,num,seed) : Samples the num of data provided from rdd,\n",
    "#     WARNING : Avoid usint it for sampling huge side data as it's executed over driver and there's no parallel processing\n",
    "rdd3.takeSample(False,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09358b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takeOrdered(n,[ordering]) : retrun given number of elements from the data in ascending order\n",
    "#     WARNING : Avoid it for huge size data, same reason as of takeSample()\n",
    "rdd3.takeOrdered(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beacf30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#takeOrdered can be computationaly expensive as we can see here what it does is,\n",
    "#if say our data is in GB then it will first convert then it returns,\n",
    "#4 elements in order, and all this is executed over driver, so no parallel processing\n",
    "rdd3.takeOrdered(4,key=lambda x:-x) # descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25bd678",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce(aggregator)\n",
    "rdd3.reduce(lambda x,y:x*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf3ec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduceByKey(aggregator):same as reduce but works on same keys\n",
    "rdd_rbk=sc.parallelize([(1,2),(1,3),(2,3),(3,4),(2,4),(5,2)],2)\n",
    "rdd_rbk.glom().collect()\n",
    "rbk=rdd_rbk.reduceByKey(lambda x,y:x+y)\n",
    "print('rdd after RBK :',rbk.collect())\n",
    "print(\"all keys :\",rbk.keys().collect())\n",
    "print(\"all values :\",rbk.values().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe518479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sortByKey(True:default/False) : Sorts with with respect to key\n",
    "print('Default :',rbk.sortByKey().collect())\n",
    "print('when False :',rbk.sortByKey(False).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fb19bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# countByKey(): Counts the number of values associated with each key\n",
    "print('Count By Key :',rdd_rbk.countByKey())\n",
    "print('In better format (key,no_of_values) :',rdd_rbk.countByKey().items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcaf839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupByKey() : \n",
    "   # WARNING : It collects all the keys and values to driver, so it's not good for big size data\n",
    "    # WHEN TO USE : when after many other operations on big size data we are getting to deal with small size data.\n",
    "rddgroup=rdd_rbk.groupByKey()\n",
    "print(\"output: \",rddgroup.collect())\n",
    "print(\"---------------------------------------------------------------------------------------------------\")\n",
    "for item in rddgroup.collect():\n",
    "    print(\"key :{key}\".format(key=item[0]),\"|\",\"values :{values}\".format(values=[item for item in item[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5849bfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lookup(key) : to see value of specific key\n",
    "rdd_rbk.lookup(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5411eb47",
   "metadata": {},
   "source": [
    "### When To use Cache or persist?\n",
    "> If we have RDD, which we needed to use many times, it's better to keep it inside a chache to avoid additional processing in storage, because once you collect you result in memory spark uses garbage collector to delete unused result, an if we have to read that RDD result again then it has to recalculate it, so to avoid this recalculation you can keep your RDD inside a Cache\n",
    "\" So if that RDD is used many times keep it in cache for faster processing \"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334e8ab1",
   "metadata": {},
   "source": [
    "> We can use cache() or persist() both for this task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e467c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_rbk.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438812b0",
   "metadata": {},
   "source": [
    "> now our RDD is in cache and it won't be deleted because of garbage collection or any storage policy running in spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeecec5a",
   "metadata": {},
   "source": [
    "### Persistance \n",
    "> When we have large size data and not that much available memory, even if we use parallel processing then also tasks will be slow, so this concept comes into picture, we distribute the data into memory and disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ba8528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "rdd1.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1ab884",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
